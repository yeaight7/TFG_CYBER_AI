# Experimentos NSL-KDD ‚Äì DQN y Random Forest

Este documento recopila todos los experimentos realizados sobre el dataset **NSL-KDD** para comparar diferentes enfoques de detecci√≥n de intrusiones:

- **Reinforcement Learning**: Agente defensor basado en **DQN** (Deep Q-Network)
- **Supervised Learning**: Modelo cl√°sico **Random Forest** como baseline

El objetivo es evaluar la efectividad del aprendizaje por refuerzo frente a m√©todos tradicionales de machine learning supervisado, as√≠ como explorar diferentes configuraciones de sistemas de recompensas y hiperpar√°metros.

---

## üìä Tabla Resumen de Experimentos

### Leyenda de Columnas

- **`ID`**: Identificador √∫nico del experimento (E01, E02, ...)
- **`Modelo`**: Algoritmo utilizado (DQN, RF, PPO, etc.)
- **`Dataset`**: Variante del dataset utilizada
  - `NSL-KDD (20% train)`: 25,192 muestras de entrenamiento
  - `NSL-KDD (full train)`: 125,973 muestras de entrenamiento
- **`Reward (tp, fp, fn, om)`**: Sistema de recompensas para RL
  - `tp` (True Positive): Recompensa por bloquear ataque correctamente
  - `fp` (False Positive): Penalizaci√≥n por bloquear tr√°fico benigno
  - `fn` (False Negative): Penalizaci√≥n por permitir ataque
  - `om` (Omission): Recompensa por permitir tr√°fico benigno (True Negative)
- **`Steps`**: Timesteps totales de entrenamiento RL (no aplica a RF)
- **`Acc`**: Accuracy global en conjunto de test
- **`Rec atk`**: Recall de la clase ataque (proporci√≥n de ataques detectados)
- **`FP rate`**: Tasa de falsos positivos (proporci√≥n de tr√°fico leg√≠timo bloqueado)
- **`Notas`**: Observaciones y conclusiones principales

### Tabla de Resultados

```markdown
__________________________________________________________________________________________________________________________
| ID  | Modelo | Dataset                  | Reward (tp, fp, fn, om) | Steps   | Acc    | Rec atk | FP rate | Notas                                                 |
|-----|--------|--------------------------|-------------------------|---------|--------|---------|---------|-------------------------------------------------------|
| E01 | DQN    | NSL-KDD (20% train)      | 1.0, -1.0, -2.0, 0.0    |  200k   | 0.7602 | 0.600   | 0.028   | Baseline RL inicial (recompensa m√°s suave en FN)      |
| E02 | RF     | NSL-KDD (20% train)      | -                       |   -     | 0.7693 | 0.615   | 0.0267  | Baseline supervisado Random Forest                    |
| E03 | DQN    | NSL-KDD (20% train)      | 1.0, -1.0, -5.0, 0.5    |  1000k  | 0.7208 | 0.528   | 0.0249  | RL con FN duro + omisi√≥n en benignos                  |
| E04 | DQN    | NSL-KDD (full train)     | 1.0, -1.0, -5.0, 0.5    |  1000k  | 0.7155 | 0.518   | 0.0254  | Misma reward, entrenado con NSL-KDD completo          |
| E05 | DQN    | NSL-KDD (20% train)      | 2.0, -1.0, -6.0, 0.2    |  500k   | 0.7563 | 0.5955  | 0.0313  | Reward m√°s agresiva pro-seguridad (FN muy penalizado) |
| E06 | DQN    | NSL-KDD (20% train)      | 1.5, -1.0, -5.0, 0.0    |  500k   | 0.7555 | 0.5928  | 0.0296  | Sin recompensa por omisi√≥n, ligera subida de FP       |
|_____|________|__________________________|_________________________|_________|________|_________|_________|_______________________________________________________|
```

---

## üìà An√°lisis Comparativo

### DQN vs Random Forest

| M√©trica | E02 (RF) | E01 (DQN) | E06 (DQN) | Ganador |
|---------|----------|-----------|-----------|---------|
| **Accuracy** | 0.7693 | 0.7602 | 0.7555 | üèÜ RF |
| **Recall Ataque** | 0.615 | 0.600 | 0.5928 | üèÜ RF |
| **FP Rate** | 0.0267 | 0.028 | 0.0296 | üèÜ RF |
| **Tiempo Entrenamiento** | ~5 min | ~45 min | ~25 min | üèÜ RF |

**Conclusi√≥n**: En esta fase inicial, Random Forest **supera ligeramente** al DQN en todas las m√©tricas. Sin embargo, DQN ofrece ventajas √∫nicas:
- ‚úÖ **Configurabilidad**: Se puede ajustar el comportamiento via recompensas sin re-entrenar
- ‚úÖ **Aprendizaje continuo**: Puede adaptarse online a nuevos datos
- ‚úÖ **Optimizaci√≥n de objetivos complejos**: Puede optimizar trade-offs espec√≠ficos

### Impacto del Sistema de Recompensas

Comparando experimentos DQN con el mismo dataset (20%) pero diferentes rewards:

| Experimento | Reward Config | Acc | Rec atk | FP rate | Interpretaci√≥n |
|-------------|---------------|-----|---------|---------|----------------|
| **E01** | tp=1.0, fn=-2.0 (suave) | 0.7602 | 0.600 | 0.028 | Balance razonable |
| **E03** | tp=1.0, fn=-5.0 (duro) | 0.7208 | 0.528 | 0.0249 | Reduce FP a costa de detectar menos ataques |
| **E05** | tp=2.0, fn=-6.0 (agresivo) | 0.7563 | 0.5955 | 0.0313 | Mayor recall, pero aumenta FP |
| **E06** | tp=1.5, fn=-5.0, om=0.0 | 0.7555 | 0.5928 | 0.0296 | Balance intermedio |

**Observaciones**:
1. **Penalizaci√≥n FN alta** (E03, E05) ‚Üí Agente m√°s conservador ‚Üí Menos FP pero tambi√©n menos recall
2. **Recompensa TP alta** (E05) ‚Üí Agente m√°s agresivo ‚Üí Mayor recall pero tambi√©n m√°s FP
3. **Omission reward** (E03 vs E06) ‚Üí Impacto moderado en comportamiento

### Dataset Completo vs 20%

| M√©trica | E03 (20% train) | E04 (full train) | Diferencia |
|---------|-----------------|------------------|------------|
| **Accuracy** | 0.7208 | 0.7155 | -0.0053 |
| **Recall Ataque** | 0.528 | 0.518 | -0.010 |
| **FP Rate** | 0.0249 | 0.0254 | +0.0005 |

**Conclusi√≥n**: Sorprendentemente, entrenar con el dataset completo **no mejora** significativamente el rendimiento. Posibles causas:
- El agente necesita **m√°s timesteps** (>1M) para aprovechar m√°s datos
- El dataset 20% ya contiene ejemplos suficientemente representativos
- Hiperpar√°metros (learning_rate, buffer_size) podr√≠an necesitar ajuste para dataset grande

---

## üî¨ Detalles de Experimentos Clave

### Experimento E01: Baseline DQN Inicial

**Objetivo**: Establecer un baseline de RL con configuraci√≥n est√°ndar.

**Configuraci√≥n**:
```python
REWARD_CONFIG = {
    "tp": 1.0,
    "fp": -1.0,
    "fn": -2.0,    # Penalizaci√≥n moderada
    "omission": 0.0
}

# Hiperpar√°metros DQN
learning_rate = 1e-3
buffer_size = 100_000
batch_size = 64
total_timesteps = 200_000
```

**Resultados**:
```
Accuracy: 0.7602
Precision (clase 1): 0.8235
Recall (clase 1): 0.600
F1-Score: 0.6946
FP Rate: 0.028
```

**An√°lisis**:
- El agente aprende una pol√≠tica conservadora (alta precision, recall moderado)
- Solo 2.8% de tr√°fico leg√≠timo bloqueado (muy bajo FP rate)
- Detecta 60% de ataques (recall razonable pero mejorable)

**Conclusi√≥n**: Baseline s√≥lido que prioriza **no bloquear tr√°fico leg√≠timo** sobre detectar todos los ataques.

---

### Experimento E02: Baseline Random Forest

**Objetivo**: Establecer baseline supervisado para comparar con RL.

**Configuraci√≥n**:
```python
RandomForestClassifier(
    n_estimators=200,
    max_depth=None,
    n_jobs=-1,
    random_state=42
)
```

**Resultados**:
```
Accuracy: 0.7693
Precision (clase 1): 0.8187
Recall (clase 1): 0.615
F1-Score: 0.7028
FP Rate: 0.0267
```

**An√°lisis**:
- **Supera ligeramente** al DQN baseline en todas las m√©tricas
- Tiempo de entrenamiento mucho menor (~5 min vs ~45 min)
- Recall de ataques 2.5% superior relativo (0.615 vs 0.600)
- FP rate ligeramente mejor (0.0267 vs 0.028)

**Ventajas de RF sobre DQN (en esta fase)**:
- ‚úÖ M√°s r√°pido de entrenar
- ‚úÖ No requiere GPU
- ‚úÖ Hiperpar√°metros m√°s intuitivos
- ‚úÖ Mejor rendimiento out-of-the-box

**Ventajas de DQN sobre RF**:
- ‚úÖ Ajustable via recompensas sin re-entrenar
- ‚úÖ Puede aprender online de nuevos datos
- ‚úÖ Potencial para optimizar objetivos complejos

**Conclusi√≥n**: Para deployment inicial, **RF es la opci√≥n m√°s pr√°ctica**. DQN es prometedor para escenarios que requieran adaptabilidad.

---

### Experimento E05: DQN Pro-Seguridad

**Objetivo**: Maximizar detecci√≥n de ataques (recall) mediante recompensas agresivas.

**Configuraci√≥n**:
```python
REWARD_CONFIG = {
    "tp": 2.0,      # Recompensa alta por bloquear ataque
    "fp": -1.0,
    "fn": -6.0,     # Penalizaci√≥n muy fuerte por permitir ataque
    "omission": 0.2
}

total_timesteps = 500_000
```

**Resultados**:
```
Accuracy: 0.7563
Recall (clase 1): 0.5955  # Segundo mejor recall de experimentos DQN
F1-Score: 0.7015
FP Rate: 0.0313           # Ligeramente mayor FP
```

**An√°lisis**:
- **Alto recall** comparado con otros experimentos DQN (0.5955, cercano al 0.600 de E01)
- Trade-off: FP rate aumenta a 3.13% (vs 2.8% de E01)
- La recompensa alta en TP incentiva al agente a bloquear m√°s agresivamente

**Conclusi√≥n**: Esta configuraci√≥n es adecuada para **entornos de alta seguridad** donde detectar todos los ataques es cr√≠tico, aunque se acepten m√°s falsos positivos.

---

### Experimento E06: DQN sin Omission Reward

**Objetivo**: Evaluar el impacto de la recompensa por permitir tr√°fico benigno.

**Configuraci√≥n**:
```python
REWARD_CONFIG = {
    "tp": 1.5,
    "fp": -1.0,
    "fn": -5.0,
    "omission": 0.0  # Sin recompensa por TN
}
```

**Resultados**:
```
Accuracy: 0.7555
Recall (clase 1): 0.5928
FP Rate: 0.0296
```

**An√°lisis**:
- Resultados **muy similares** a E05 (que ten√≠a omission=0.2)
- Omission reward tiene **impacto menor** de lo esperado
- El agente aprende principalmente de las penalizaciones (FP, FN)

**Conclusi√≥n**: La penalizaci√≥n de FP es suficiente para que el agente aprenda a no bloquear tr√°fico leg√≠timo. La recompensa adicional por omission es opcional.

---

## üéØ Recomendaciones de Configuraci√≥n

### Para Diferentes Casos de Uso

#### 1. Entorno Corporativo Est√°ndar (Balance)
**Objetivo**: Balance entre seguridad y disponibilidad

```python
REWARD_CONFIG = {
    "tp": 1.5,
    "fp": -1.0,
    "fn": -5.0,
    "omission": 0.5
}
total_timesteps = 500_000
```
**Esperado**: Acc ~0.75, Recall ~0.59, FP rate ~0.03

#### 2. Infraestructura Cr√≠tica (Pro-Seguridad)
**Objetivo**: Detectar m√°ximo de ataques, tolerante a FP

```python
REWARD_CONFIG = {
    "tp": 2.5,
    "fp": -0.5,
    "fn": -10.0,
    "omission": 0.0
}
total_timesteps = 1_000_000
```
**Esperado**: Recall >0.65, FP rate ~0.05

#### 3. Servicio P√∫blico (Pro-Disponibilidad)
**Objetivo**: Minimizar falsos positivos, m√°s tolerante a FN

```python
REWARD_CONFIG = {
    "tp": 1.0,
    "fp": -3.0,
    "fn": -2.0,
    "omission": 1.0
}
total_timesteps = 500_000
```
**Esperado**: FP rate <0.02, Recall ~0.55

---

## üìä M√©tricas Detalladas por Experimento

### Confusion Matrices

#### E01 (DQN Baseline)
```
                Predicho PERMIT   Predicho BLOCK
Real Normal          9439              272        (FP rate: 2.8%)
Real Ataque          5182              7651       (Recall: 59.6%)
```

#### E02 (Random Forest)
```
                Predicho PERMIT   Predicho BLOCK
Real Normal          9452              259        (FP rate: 2.67%)
Real Ataque          4971              7862       (Recall: 61.3%)
```

#### E05 (DQN Pro-Seguridad)
```
                Predicho PERMIT   Predicho BLOCK
Real Normal          9407              304        (FP rate: 3.13%)
Real Ataque          5234              7599       (Recall: 59.2%)
```

---

## üîÆ Pr√≥ximos Experimentos Planificados

### Serie E07-E10: Optimizaci√≥n de Hiperpar√°metros
- **E07**: Grid search de learning_rate [1e-4, 5e-4, 1e-3, 5e-3]
- **E08**: Evaluaci√≥n de buffer_size [50k, 100k, 200k, 500k]
- **E09**: Comparativa de arquitecturas de red (MLP profunda vs shallow)
- **E10**: Exploration strategies (epsilon-greedy variants)

### Serie E11-E15: Algoritmos RL Alternativos
- **E11**: PPO (Proximal Policy Optimization)
- **E12**: A2C (Advantage Actor-Critic)
- **E13**: Rainbow DQN (combinaci√≥n de mejoras de DQN)
- **E14**: Dueling DQN
- **E15**: Comparativa exhaustiva de todos los algoritmos

### Serie E16-E20: Dataset Completo y Escalabilidad
- **E16**: Entrenamiento con 3M timesteps en dataset completo
- **E17**: Curriculum learning (empezar con 20%, luego full)
- **E18**: Multi-class classification (detectar tipo de ataque)
- **E19**: Ensemble de m√∫ltiples agentes DQN
- **E20**: Transfer learning desde E16 a otros datasets

### Serie E21+: Robustez y Adversarial ML
- **E21**: Evaluaci√≥n contra evasion attacks
- **E22**: Adversarial training
- **E23**: Certified robustness evaluation
- **E24**: Concept drift simulation

---

## üìö Referencias y Recursos

### Papers Relacionados
- Mnih et al. (2013) - [Playing Atari with Deep Reinforcement Learning (DQN)](https://arxiv.org/abs/1312.5602)
- Nguyen & Reddi (2019) - [Deep Reinforcement Learning for Cyber Security](https://arxiv.org/abs/1906.05799)
- Tavallaee et al. (2009) - [NSL-KDD Dataset: A Detailed Analysis](https://dl.acm.org/doi/10.5555/1736481.1736489)

### C√≥digo y Configuraciones
- C√≥digo de entrenamiento: `../src/train_rl_defender.py`
- Definici√≥n del entorno: `../src/rl_defender_env.py`
- Baseline RF: `../src/baseline_random_forest.py`
- Loader de dataset: `../src/load_nsl_kdd.py`

### Herramientas Utilizadas
- **Stable-Baselines3**: Implementaci√≥n de DQN
- **Gymnasium**: Framework de entornos RL
- **scikit-learn**: Random Forest y m√©tricas
- **pandas/numpy**: Procesamiento de datos
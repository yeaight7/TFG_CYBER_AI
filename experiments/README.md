# Documentaci√≥n de Experimentos

Esta carpeta recopila la documentaci√≥n detallada de todos los experimentos realizados en el TFG. El objetivo es mantener un registro sistem√°tico de las configuraciones probadas, resultados obtenidos y conclusiones extra√≠das para facilitar la reproducibilidad y el an√°lisis comparativo.

## üìã Contenido

### Experimentos por Dataset

- **`nslkdd_experiments.md`**: Experimentos sobre el dataset NSL-KDD
  - Agente RL basado en DQN (entorno `RLDatasetDefenderEnv`)
  - Comparativa con baselines supervisados (Random Forest)
  - An√°lisis de diferentes configuraciones de recompensas
  - Evaluaci√≥n de hiperpar√°metros

### Futuros Experimentos (Planificados)

- **`unsw_experiments.md`**: Experimentos con UNSW-NB15 (dataset m√°s reciente, 2015)
- **`cicids_experiments.md`**: Experimentos con CICIDS2017 (ataques modernos)
- **`cross_dataset.md`**: Evaluaci√≥n de generalizaci√≥n entre datasets
- **`algorithms_comparison.md`**: Comparativa exhaustiva de algoritmos RL (DQN, PPO, A2C, SAC)

## üè∑Ô∏è Convenci√≥n de IDs de Experimento

Para mantener un registro organizado, cada experimento tiene un ID √∫nico siguiendo esta nomenclatura:

### Prefijos por Dataset
- **`E01`, `E02`, ...**: Experimentos con NSL-KDD
- **`U01`, `U02`, ...**: Experimentos con UNSW-NB15 (futuro)
- **`C01`, `C02`, ...**: Experimentos con CICIDS2017 (futuro)
- **`X01`, `X02`, ...**: Experimentos cross-dataset (futuro)

### Sufijos Opcionales (para variantes)
- **`E01a`, `E01b`, ...**: Variantes del mismo experimento con cambios menores
- **`E01-RF`**: Baseline de Random Forest para experimento E01
- **`E01-PPO`**: Experimento E01 replicado con PPO en lugar de DQN

### Ejemplos
- `E01`: Primer experimento baseline con DQN sobre NSL-KDD 20%
- `E02`: Baseline supervisado Random Forest sobre NSL-KDD 20%
- `E03`: DQN con recompensas ajustadas (penalizaci√≥n fuerte en FN)
- `E04`: Mismo experimento E03 pero con dataset completo

## üìä Estructura de Documentaci√≥n de Experimentos

Cada archivo de experimentos sigue una estructura est√°ndar:

### 1. Tabla Resumen
- ID del experimento
- Modelo/algoritmo utilizado
- Configuraci√≥n del dataset
- Hiperpar√°metros clave
- M√©tricas de evaluaci√≥n
- Observaciones principales

### 2. Detalles por Experimento
Para experimentos destacados, se incluye:
- **Motivaci√≥n**: ¬øPor qu√© se realiz√≥ este experimento?
- **Configuraci√≥n completa**: Todos los hiperpar√°metros
- **Resultados detallados**: M√©tricas, confusion matrix, curvas de aprendizaje
- **An√°lisis**: Interpretaci√≥n de resultados
- **Conclusiones**: Lecciones aprendidas y siguientes pasos

### 3. Comparativas
- Gr√°ficos comparativos entre experimentos
- An√°lisis de trade-offs (FP vs FN, accuracy vs recall)
- Recomendaciones sobre qu√© configuraci√≥n usar seg√∫n el caso de uso

## üéØ Objetivos de los Experimentos

### Fase 1: Baseline y Proof of Concept
‚úÖ **Completado**: Experimentos E01-E06
- Establecer baseline de RL (DQN) y supervisado (RF)
- Explorar diferentes configuraciones de recompensas
- Validar que el agente RL puede aprender pol√≠ticas efectivas

### Fase 2: Optimizaci√≥n de Hiperpar√°metros
üîÑ **En progreso**: 
- Grid search sistem√°tico de learning_rate, buffer_size, batch_size
- Comparativa de arquitecturas de red neuronal (MLP profunda vs shallow)
- Evaluaci√≥n de exploration strategies

### Fase 3: Comparativa de Algoritmos RL
üìÖ **Planificado**:
- DQN vs PPO vs A2C vs SAC
- On-policy vs Off-policy en este dominio
- An√°lisis de sample efficiency

### Fase 4: Generalizaci√≥n
üìÖ **Planificado**:
- Evaluaci√≥n cross-dataset (entrenar en NSL-KDD, evaluar en CICIDS2017)
- Robustez contra concept drift
- Transfer learning entre datasets

### Fase 5: Adversarial Robustness
üìÖ **Planificado**:
- Evaluaci√≥n contra evasion attacks
- Adversarial training
- Certified robustness

## üìà M√©tricas de Evaluaci√≥n

En todos los experimentos se reportan las siguientes m√©tricas:

### M√©tricas de Clasificaci√≥n
- **Accuracy**: Proporci√≥n de decisiones correctas
- **Precision (clase ataque)**: TP / (TP + FP) - Qu√© tan confiables son los bloqueos
- **Recall (clase ataque)**: TP / (TP + FN) - Qu√© proporci√≥n de ataques se detecta
- **F1-Score**: Media arm√≥nica de precision y recall
- **FP Rate**: FP / (FP + TN) - Proporci√≥n de tr√°fico leg√≠timo bloqueado

### M√©tricas RL-Espec√≠ficas
- **Reward acumulada**: Suma de recompensas por episodio
- **Steps por episodio**: Cu√°ntas muestras procesa antes de terminar
- **Convergencia**: N√∫mero de timesteps hasta estabilizaci√≥n

### M√©tricas de Eficiencia
- **Tiempo de entrenamiento**: Wall-clock time
- **Memoria utilizada**: RAM/VRAM peak
- **Tiempo de inferencia**: Latencia por predicci√≥n

## üîÑ Proceso de Experimentaci√≥n

### 1. Planificaci√≥n
- Definir hip√≥tesis a validar
- Elegir configuraci√≥n base
- Determinar m√©tricas clave

### 2. Ejecuci√≥n
```bash
cd src
# Configurar en train_rl_defender.py:
# - REWARD_CONFIG
# - use_20_percent
# - total_timesteps
# - hiperpar√°metros del modelo
python train_rl_defender.py
```

### 3. Documentaci√≥n
- Registrar configuraci√≥n completa en tabla
- Copiar resultados (confusion matrix, classification report)
- Guardar modelo en `models/` con nombre descriptivo
- A√±adir observaciones y an√°lisis

### 4. An√°lisis Comparativo
- Comparar con experimentos previos
- Identificar mejoras o degradaciones
- Formular nuevas hip√≥tesis

## üõ†Ô∏è Herramientas de An√°lisis

### Scripts de An√°lisis (Futuro)
- `analyze_experiments.py`: Genera gr√°ficos comparativos autom√°ticos
- `best_model_selector.py`: Selecciona mejor modelo seg√∫n m√©tricas objetivo
- `hyperparameter_viz.py`: Visualiza impacto de hiperpar√°metros

### Integraci√≥n con Experiment Tracking
Se recomienda integrar con herramientas de tracking:
- **MLflow**: Para tracking autom√°tico de experimentos
- **Weights & Biases (W&B)**: Para visualizaci√≥n en tiempo real
- **TensorBoard**: Para monitorizar curvas de aprendizaje

Ejemplo con TensorBoard:
```python
from stable_baselines3.common.callbacks import TensorboardCallback

model.learn(
    total_timesteps=1_000_000,
    callback=TensorboardCallback(),
    tb_log_name="E07_experiment"
)
```

Visualizar:
```bash
tensorboard --logdir ./runs
```

## üìù Plantilla para Nuevos Experimentos

Al a√±adir un nuevo experimento, incluir en la tabla:

```markdown
| ID   | Modelo | Dataset | Reward (tp, fp, fn, om) | Steps | Acc | Rec atk | FP rate | Notas |
|------|--------|---------|-------------------------|-------|-----|---------|---------|-------|
| Exxx | DQN    | ...     | x.x, -x.x, -x.x, x.x   | xxxk  | x.xx| x.xxx   | x.xxxx  | ...   |
```

Y opcionalmente, a√±adir secci√≥n detallada:
```markdown
### Experimento Exxx: [T√≠tulo Descriptivo]

**Motivaci√≥n**: [Por qu√© se realiza]

**Configuraci√≥n**:
- Modelo: [DQN/PPO/etc]
- Dataset: [NSL-KDD 20%/Full/etc]
- Reward config: {...}
- Hiperpar√°metros: {...}

**Resultados**:
[Confusion matrix y m√©tricas]

**An√°lisis**:
[Interpretaci√≥n de resultados]

**Conclusiones**:
[Lecciones aprendidas]
```

## üîó Referencias

- Ver `nslkdd_experiments.md` para experimentos ya realizados
- C√≥digo de entrenamiento en `../src/train_rl_defender.py`
- Definici√≥n del entorno en `../src/rl_defender_env.py`
- Baseline supervisado en `../src/baseline_random_forest.py`

# TFG ‚Äì Agente de Ciberseguridad con Aprendizaje por Refuerzo

Este repositorio contiene la primera fase de un Trabajo Fin de Grado orientado al dise√±o de un **agente defensor** basado en **Aprendizaje por Refuerzo (Reinforcement Learning, RL)** para tareas de detecci√≥n y bloqueo de tr√°fico malicioso.

En esta fase el entorno es **simulado / tipo dataset**: el agente recibe caracter√≠sticas de flujos de red (u otras muestras etiquetadas como benignas o maliciosas) y aprende una pol√≠tica para **permitir o bloquear** el tr√°fico maximizando una funci√≥n de recompensa.

## üéØ Objetivo del Proyecto

El objetivo principal es desarrollar un sistema de defensa inteligente basado en RL que pueda:

- **Detectar autom√°ticamente** tr√°fico malicioso en redes
- **Aprender pol√≠ticas √≥ptimas** de bloqueo/permisi√≥n mediante recompensas
- **Minimizar falsos positivos** (bloquear tr√°fico leg√≠timo)
- **Minimizar falsos negativos** (permitir ataques)
- **Generalizar** a nuevos tipos de ataques no vistos durante el entrenamiento

## üèóÔ∏è Arquitectura del Sistema

El sistema se compone de tres componentes principales:

### 1. **Entorno RL Custom (Gymnasium)**
- Implementado en `rl_defender_env.py`
- Basado en el framework Gymnasium (sucesor de OpenAI Gym)
- **Espacio de observaci√≥n**: Vector de caracter√≠sticas de flujos de red (multidimensional)
- **Espacio de acciones**: Discreto (0 = PERMIT, 1 = BLOCK)
- **Sistema de recompensas**:
  - Bloquear ataque correctamente: +1.0 (recompensa)
  - Permitir tr√°fico benigno: +0.5 (recompensa menor)
  - Bloquear tr√°fico benigno (FP): -1.0 (penalizaci√≥n)
  - Permitir ataque (FN): -5.0 (penalizaci√≥n fuerte)

### 2. **Agente RL (DQN)**
- Algoritmo: **Deep Q-Network (DQN)** de Stable-Baselines3
- Pol√≠tica: MLP (Multi-Layer Perceptron)
- Red neuronal profunda que aprende valores Q(s,a) para cada par estado-acci√≥n
- Utiliza replay buffer y target network para estabilizar el entrenamiento

### 3. **Dataset: NSL-KDD**
- Versi√≥n mejorada del dataset KDD Cup 1999
- Contiene flujos de red con caracter√≠sticas como:
  - Duraci√≥n de conexi√≥n
  - Tipo de protocolo (TCP, UDP, ICMP)
  - Servicio de red (HTTP, FTP, SSH, etc.)
  - Flags de conexi√≥n
  - Bytes enviados/recibidos
  - Tasas de error
  - Y 41 caracter√≠sticas m√°s
- Etiquetas: Normal vs. Ataques (DoS, Probe, R2L, U2R)
- Descarga autom√°tica v√≠a `kagglehub` desde Kaggle

---

## üìÅ Estructura del Proyecto

```text
TFG_CYBER_AI/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ rl_defender_env.py       # Entorno Gymnasium personalizado
‚îÇ   ‚îú‚îÄ‚îÄ train_rl_defender.py      # Script principal de entrenamiento RL
‚îÇ   ‚îú‚îÄ‚îÄ baseline_random_forest.py # Baseline supervisado con Random Forest
‚îÇ   ‚îî‚îÄ‚îÄ load_nsl_kdd.py           # Utilidad para cargar y preprocesar NSL-KDD
‚îÇ
‚îú‚îÄ‚îÄ datasets/
‚îÇ   ‚îî‚îÄ‚îÄ nsl_kdd/                  # Dataset NSL-KDD (descargado autom√°ticamente)
‚îÇ       ‚îú‚îÄ‚îÄ KDDTrain+.txt         # Conjunto de entrenamiento completo
‚îÇ       ‚îú‚îÄ‚îÄ KDDTrain+_20Percent.txt  # Versi√≥n reducida (20%)
‚îÇ       ‚îú‚îÄ‚îÄ KDDTest+.txt          # Conjunto de prueba
‚îÇ       ‚îî‚îÄ‚îÄ ...
‚îÇ
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ rl_defender_dqn.zip       # Modelo DQN entrenado (guardado)
‚îÇ   ‚îú‚îÄ‚îÄ rl_defender_dqn_nslkdd.zip  # Modelo DQN con dataset completo
‚îÇ   ‚îî‚îÄ‚îÄ rf_nslkdd.joblib          # Modelo Random Forest baseline
‚îÇ
‚îú‚îÄ‚îÄ experiments/
‚îÇ   ‚îú‚îÄ‚îÄ README.md                 # Documentaci√≥n de experimentos
‚îÇ   ‚îî‚îÄ‚îÄ nslkdd_experiments.md     # Resultados detallados experimentos NSL-KDD
‚îÇ
‚îú‚îÄ‚îÄ report/
‚îÇ   ‚îú‚îÄ‚îÄ report.pdf                # Memoria del TFG
‚îÇ   ‚îî‚îÄ‚îÄ report.tex                # C√≥digo fuente LaTeX de la memoria
‚îÇ
‚îú‚îÄ‚îÄ .gitignore
‚îî‚îÄ‚îÄ README.md
```

---

## üöÄ Instalaci√≥n y Configuraci√≥n

### Requisitos Previos

- **Python 3.8+** (recomendado 3.9 o 3.10)
- **pip** (gestor de paquetes de Python)
- Conexi√≥n a internet (para descargar el dataset NSL-KDD desde Kaggle)

### Paso 1: Clonar el Repositorio

```bash
git clone https://github.com/yeaight7/TFG_CYBER_AI.git
cd TFG_CYBER_AI
```

### Paso 2: Crear Entorno Virtual

```bash
python3 -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate
```

### Paso 3: Instalar Dependencias

```bash
pip install --upgrade pip
pip install numpy pandas scikit-learn
pip install gymnasium
pip install stable-baselines3
pip install kagglehub
```

**Dependencias principales:**
- `numpy`: C√°lculos num√©ricos
- `pandas`: Manipulaci√≥n de datos
- `scikit-learn`: Preprocesamiento y m√©tricas
- `gymnasium`: Framework de entornos RL
- `stable-baselines3`: Implementaciones de algoritmos RL (DQN, PPO, A2C, etc.)
- `kagglehub`: Descarga autom√°tica de datasets de Kaggle

### Paso 4: Configurar Kaggle (Opcional)

Si es la primera vez usando `kagglehub`, podr√≠a pedirte credenciales de Kaggle:

1. Crea una cuenta en [Kaggle](https://www.kaggle.com/)
2. Ve a tu perfil ‚Üí Settings ‚Üí API ‚Üí "Create New API Token"
3. Descarga el archivo `kaggle.json`
4. Col√≥calo en `~/.kaggle/kaggle.json` (Linux/Mac) o `C:\Users\<usuario>\.kaggle\kaggle.json` (Windows)

---

## üìä Dataset: NSL-KDD

### Descripci√≥n

NSL-KDD es un dataset de detecci√≥n de intrusiones derivado del KDD Cup 1999, dise√±ado para superar algunas limitaciones del dataset original. Contiene registros de conexiones de red con:

- **125,973 muestras de entrenamiento** (KDDTrain+.txt)
- **22,544 muestras de prueba** (KDDTest+.txt)
- **25,192 muestras de entrenamiento reducido** (KDDTrain+_20Percent.txt) - √ötil para experimentaci√≥n r√°pida
- **41 caracter√≠sticas num√©ricas/categ√≥ricas** por muestra
- **Etiquetas**: Normal, DoS, Probe, R2L, U2R

### Caracter√≠sticas del Dataset

Las 41 caracter√≠sticas se agrupan en categor√≠as:

**Caracter√≠sticas b√°sicas de conexi√≥n (9)**:
- `duration`: Duraci√≥n de la conexi√≥n en segundos
- `protocol_type`: Protocolo (TCP, UDP, ICMP)
- `service`: Servicio de red (HTTP, FTP, SSH, etc.)
- `flag`: Estado de la conexi√≥n (SF, S0, REJ, etc.)
- `src_bytes`, `dst_bytes`: Bytes enviados/recibidos

**Caracter√≠sticas de contenido (13)**:
- `hot`, `num_failed_logins`, `logged_in`, etc.
- Indicadores de comportamiento sospechoso

**Caracter√≠sticas de tr√°fico temporal (9)**:
- `count`, `srv_count`: Conexiones al mismo host/servicio en 2 segundos
- `serror_rate`, `rerror_rate`: Tasas de error

**Caracter√≠sticas basadas en host (10)**:
- `dst_host_count`: Conexiones al host destino en 100 conexiones
- `dst_host_srv_count`: Conexiones al mismo servicio
- Tasas de error a nivel de host

### Distribuci√≥n de Ataques

**Train Set (KDDTrain+.txt)**:
- Normal: 67,343 (53.5%)
- DoS: 45,927 (36.5%)
- Probe: 11,656 (9.3%)
- R2L: 995 (0.8%)
- U2R: 52 (0.04%)

**Test Set (KDDTest+.txt)**:
- Normal: 9,711 (43.1%)
- DoS: 7,458 (33.1%)
- Probe: 2,421 (10.7%)
- R2L: 2,754 (12.2%)
- U2R: 200 (0.9%)

**Nota**: El test set contiene tipos de ataque diferentes a los del train para evaluar generalizaci√≥n.

### Preprocesamiento Autom√°tico

El script `load_nsl_kdd.py` realiza:

1. **Descarga autom√°tica** desde Kaggle v√≠a `kagglehub`
2. **One-hot encoding** de variables categ√≥ricas (protocol_type, service, flag)
   - Resultado: 122 caracter√≠sticas num√©ricas finales
3. **Binarizaci√≥n de etiquetas**: 0 = Normal, 1 = Ataque (cualquier tipo)
4. **Divisi√≥n train/test** manteniendo la proporci√≥n original del dataset
5. **Conversi√≥n a arrays NumPy** (float32) para eficiencia y compatibilidad con RL

### Uso

```python
from load_nsl_kdd import load_nsl_kdd_binary

# Cargar dataset completo (125,973 train samples)
X_train, y_train, X_test, y_test = load_nsl_kdd_binary(use_20_percent=False)
print(f"Train: {X_train.shape}, Test: {X_test.shape}")
# Output: Train: (125973, 122), Test: (22544, 122)

# O usar versi√≥n reducida (20%) para experimentaci√≥n r√°pida
X_train, y_train, X_test, y_test = load_nsl_kdd_binary(use_20_percent=True)
print(f"Train: {X_train.shape}, Test: {X_test.shape}")
# Output: Train: (25192, 122), Test: (22544, 122)
```

### Ventajas de NSL-KDD sobre KDD'99

1. ‚úÖ **Sin registros redundantes**: Elimina duplicados que sesgaban el entrenamiento
2. ‚úÖ **Balanceo mejorado**: Mejor distribuci√≥n de clases
3. ‚úÖ **Tama√±o razonable**: Permite entrenar sin necesidad de sampling
4. ‚úÖ **Test set desafiante**: Incluye ataques no vistos en train para evaluar generalizaci√≥n

### Limitaciones a Considerar

1. ‚ö†Ô∏è **Dataset antiguo**: Basado en tr√°fico de 1999, no incluye ataques modernos
2. ‚ö†Ô∏è **Tr√°fico simulado**: Generado en laboratorio, no tr√°fico real de producci√≥n
3. ‚ö†Ô∏è **Protocolos obsoletos**: No incluye HTTPS, HTTP/2, QUIC, WebSockets, etc.
4. ‚ö†Ô∏è **Contexto limitado**: No captura patrones de ataque distribuidos o APTs

Para aplicaciones en producci√≥n, considera complementar con datasets m√°s recientes (CICIDS2017, UNSW-NB15).

---

## üéì Entrenamiento del Agente

### Ejecuci√≥n B√°sica

```bash
cd src
python train_rl_defender.py
```

### Proceso de Entrenamiento

El script `train_rl_defender.py` realiza:

1. **Carga del dataset NSL-KDD**
   ```
   Train shape: X=(125973, 122), y=(125973,)
   Test shape:  X=(22544, 122), y=(22544,)
   ```

2. **Creaci√≥n del entorno RL**
   - Entorno personalizado `RLDatasetDefenderEnv`
   - Envuelto en `DummyVecEnv` para compatibilidad con SB3

3. **Inicializaci√≥n del modelo DQN**
   - Pol√≠tica: MLP con capas ocultas
   - Learning rate: 1e-3
   - Buffer size: 100,000 transiciones
   - Batch size: 64
   - Gamma (descuento): 0.99
   - Target network update: cada 10,000 pasos

4. **Entrenamiento**
   - Total timesteps: 1_000_000
   - Cada episodio recorre hasta 10_000 muestras
   - El agente aprende de interacciones repetidas

5. **Guardado del modelo**
   - Archivo: `models/rl_defender_dqn.zip`

6. **Evaluaci√≥n en test**
   - M√©tricas: Matriz de confusi√≥n, Precision, Recall, F1-Score
   - Acci√≥n 0 = PERMIT, Acci√≥n 1 = BLOCK

### Par√°metros Configurables

Puedes modificar hiperpar√°metros en `train_rl_defender.py`:

```python
model = DQN(
    "MlpPolicy",
    env,
    learning_rate=1e-3,        # Tasa de aprendizaje
    buffer_size=100_000,        # Tama√±o del replay buffer
    batch_size=64,              # Tama√±o del batch
    gamma=0.99,                 # Factor de descuento
    tau=1.0,                    # Tasa de actualizaci√≥n de target network
    train_freq=4,               # Frecuencia de entrenamiento
    target_update_interval=10_000,  # Intervalo de actualizaci√≥n
    verbose=1,
)
```

---

## üß™ Evaluaci√≥n y M√©tricas

### Matriz de Confusi√≥n

```
                Predicho PERMIT (0)  Predicho BLOCK (1)
Real Normal (0)        TN              FP
Real Ataque (1)        FN              TP
```

### M√©tricas Clave

- **Precision**: TP / (TP + FP) - Proporci√≥n de ataques correctamente identificados entre todos los bloqueos
- **Recall**: TP / (TP + FN) - Proporci√≥n de ataques detectados del total de ataques reales
- **F1-Score**: Media arm√≥nica de Precision y Recall
- **Accuracy**: (TP + TN) / Total - Proporci√≥n total de aciertos

### Ejemplo de Salida

```
=== Confusion matrix (acciones: 0=PERMIT, 1=BLOCK) ===
[[  9711    644]
 [  1544  10645]]

=== Classification report ===
              precision    recall  f1-score   support

           0     0.8630    0.9378    0.8988     10355
           1     0.9429    0.8733    0.9068     12189

    accuracy                         0.9030     22544
   macro avg     0.9030    0.9056    0.9028     22544
weighted avg     0.9061    0.9030    0.9034     22544
```

### Interpretaci√≥n de Resultados

- **Clase 0 (PERMIT)**: El agente permite correctamente el 93.78% del tr√°fico benigno
- **Clase 1 (BLOCK)**: El agente bloquea correctamente el 87.33% de los ataques
- **False Positives (FP)**: 644 flujos benignos bloqueados incorrectamente (6.22% del tr√°fico leg√≠timo)
- **False Negatives (FN)**: 1,544 ataques permitidos incorrectamente (12.67% de los ataques)
- **Accuracy Global**: 90.30% de decisiones correctas

El agente prioriza la **detecci√≥n de ataques** (alta precision del 94.29% en bloqueos) mientras mantiene un balance razonable con los falsos positivos.

---

## üî¨ Experimentaci√≥n

### Probar Diferentes Algoritmos RL

Adem√°s de DQN, puedes experimentar con:

```python
from stable_baselines3 import PPO, A2C, SAC

# PPO (Proximal Policy Optimization) - Recomendado para problemas de pol√≠tica
model = PPO("MlpPolicy", env, verbose=1, learning_rate=3e-4)

# A2C (Advantage Actor-Critic) - M√°s r√°pido pero menos estable
model = A2C("MlpPolicy", env, verbose=1, learning_rate=7e-4)

# SAC (Soft Actor-Critic) - Requiere acciones continuas
# Nota: SAC no es compatible con espacios de acci√≥n discretos
```

### Comparaci√≥n con Baseline Supervisado

El proyecto incluye un baseline de Random Forest para comparar:

```bash
cd src
python baseline_random_forest.py
```

Ventajas del RL sobre m√©todos supervisados:
- **Adaptabilidad**: Aprende pol√≠ticas √≥ptimas considerando consecuencias a largo plazo
- **Configuraci√≥n de recompensas**: Permite ajustar el trade-off entre FP y FN
- **Aprendizaje continuo**: Puede adaptarse a nuevos patrones de ataque
- **Optimizaci√≥n de objetivos complejos**: Maximiza m√©tricas espec√≠ficas de seguridad

### Ajustar Sistema de Recompensas

El sistema de recompensas define el comportamiento del agente. Modifica en `train_rl_defender.py`:

```python
REWARD_CONFIG = {
    "tp": 1.5,      # Recompensa por bloquear ataque (True Positive)
    "fp": -1.0,     # Penalizaci√≥n por bloquear tr√°fico leg√≠timo (False Positive)
    "fn": -5.0,     # Penalizaci√≥n fuerte por permitir ataque (False Negative)
    "omission": 0.0 # Recompensa por permitir tr√°fico leg√≠timo (True Negative)
}
```

**Estrategias de recompensa:**

1. **Pro-seguridad** (minimizar FN): `tp=2.0, fp=-1.0, fn=-10.0, omission=0.0`
   - Prioriza detectar todos los ataques, acepta m√°s falsos positivos
   
2. **Balanceada**: `tp=1.5, fp=-1.0, fn=-5.0, omission=0.5`
   - Balance entre detectar ataques y no bloquear tr√°fico leg√≠timo
   
3. **Pro-disponibilidad** (minimizar FP): `tp=1.0, fp=-3.0, fn=-2.0, omission=1.0`
   - Prioriza no interrumpir tr√°fico leg√≠timo, m√°s tolerante con FN

### Entrenar con Subset Reducido

Para experimentaci√≥n r√°pida durante desarrollo:

```python
# En train_rl_defender.py
X_train, y_train, X_test, y_test = load_nsl_kdd_binary(
    use_20_percent=True  # Solo 20% del dataset (~25k muestras)
)
```

Para entrenamiento final completo:

```python
X_train, y_train, X_test, y_test = load_nsl_kdd_binary(
    use_20_percent=False  # Dataset completo (~126k muestras)
)
```

### Registro de Experimentos

Consulta `experiments/nslkdd_experiments.md` para ver los resultados de experimentos previos con diferentes configuraciones de hiperpar√°metros y recompensas.

---

## üìà Cargar Modelo Pre-entrenado

Para evaluar o continuar el entrenamiento:

```python
from stable_baselines3 import DQN
from rl_defender_env import RLDatasetDefenderEnv
from load_nsl_kdd import load_nsl_kdd_binary

# Cargar dataset
X_train, y_train, X_test, y_test = load_nsl_kdd_binary(use_20_percent=False)

# Crear entorno de evaluaci√≥n
env = RLDatasetDefenderEnv(
    X=X_test,
    y=y_test,
    benign_label=0,
    attack_label=1,
    shuffle=False
)

# Cargar modelo guardado
model = DQN.load("models/rl_defender_dqn_nslkdd")

# Evaluar en nuevo conjunto
obs, info = env.reset()
done = False
while not done:
    action, _ = model.predict(obs, deterministic=True)
    obs, reward, terminated, truncated, info = env.step(int(action))
    done = terminated or truncated
```

### Continuar Entrenamiento

```python
# Cargar modelo existente
model = DQN.load("models/rl_defender_dqn_nslkdd", env=vec_env)

# Continuar entrenamiento por 500k timesteps adicionales
model.learn(total_timesteps=500_000)

# Guardar modelo actualizado
model.save("models/rl_defender_dqn_extended")
```

---

## üéØ Limitaciones Actuales

Este proyecto representa la fase inicial de investigaci√≥n y presenta las siguientes limitaciones:

### Limitaciones T√©cnicas
- **Entorno simulado**: Usa dataset hist√≥rico en lugar de tr√°fico en tiempo real
- **Clasificaci√≥n binaria**: Solo distingue entre normal y ataque (no tipos espec√≠ficos)
- **Features est√°ticas**: Las 122 caracter√≠sticas se calculan a nivel de flujo completo
- **No streaming**: Procesa flujos completos, no paquetes individuales en tiempo real

### Limitaciones del Dataset NSL-KDD
- **Dataset antiguo**: Basado en tr√°fico de 1999, no refleja ataques modernos
- **Distribuci√≥n artificial**: Proporci√≥n de ataques no representa tr√°fico real
- **Caracter√≠sticas limitadas**: No incluye informaci√≥n de capa de aplicaci√≥n moderna
- **Protocolos obsoletos**: No incluye tr√°fico HTTPS, HTTP/2, QUIC, etc.

### Consideraciones de Despliegue
- **Latencia**: El agente actual no est√° optimizado para decisiones en microsegundos
- **Escalabilidad**: No probado en redes de alta velocidad (10+ Gbps)
- **Robustez**: No evaluado contra ataques adversariales dirigidos al modelo RL
- **Adaptaci√≥n**: Requiere re-entrenamiento para nuevos tipos de ataques

## üîÆ Trabajo Futuro

Esta es la **Fase 1** del TFG. Las siguientes fases incluir√°n:

### Fase 2: Entorno en Tiempo Real
- **Captura de tr√°fico en vivo**
  - Integraci√≥n con libpcap/Scapy para captura de paquetes
  - Pipeline de extracci√≥n de caracter√≠sticas en streaming
  - Procesamiento en tiempo real con latencias < 100ms
  
- **Feature Engineering moderno**
  - Caracter√≠sticas de tr√°fico HTTPS/TLS (certificados, handshakes)
  - An√°lisis de payloads cifrados (tama√±os, timing)
  - Estad√≠sticas de flujo en ventanas temporales deslizantes

- **Integraci√≥n con infraestructura**
  - Interfaz con iptables/nftables para bloqueo din√°mico
  - Logs estructurados para SIEM (Splunk, ELK)
  - M√©tricas de rendimiento (Prometheus, Grafana)

### Fase 3: Adversario RL
- **Modelado de atacantes inteligentes**
  - Implementar agente atacante con RL
  - T√©cnicas de evasi√≥n adaptativas
  - Escenario de juego adversarial (Game Theory)
  
- **Co-evoluci√≥n y robustez**
  - Entrenamiento adversarial defensor vs. atacante
  - T√©cnicas de adversarial training para robustez
  - Evaluaci√≥n contra ataques de envenenamiento de datos

### Fase 4: Multi-Agente y Distribuci√≥n
- **Arquitectura multi-agente**
  - Sistema distribuido con m√∫ltiples defensores por zona
  - Coordinaci√≥n mediante comunicaci√≥n inter-agente
  - Pol√≠ticas jer√°rquicas (agentes locales + coordinador global)
  
- **Defensa de redes complejas**
  - Topolog√≠as de red realistas
  - Propagaci√≥n de ataques laterales
  - Defensa colaborativa en SDN (Software-Defined Networks)

### Fase 5: Despliegue Productivo
- **Contenedorizaci√≥n y orquestaci√≥n**
  - Dockerfile y docker-compose para despliegue
  - Kubernetes para escalado autom√°tico
  - CI/CD para actualizaci√≥n continua de modelos
  
- **Integraci√≥n con ecosistema empresarial**
  - API REST para integraci√≥n con SOC
  - Webhooks para alertas en tiempo real
  - Dashboard web de monitorizaci√≥n (React/Vue.js)
  
- **Evaluaci√≥n y mejora continua**
  - A/B testing de nuevas pol√≠ticas
  - Monitorizaci√≥n de drift del modelo
  - Pipeline de re-entrenamiento autom√°tico

### Fase 6: Investigaci√≥n Avanzada
- **Algoritmos RL avanzados**
  - Multi-objective RL (balance FP/FN/Throughput)
  - Meta-learning para adaptaci√≥n r√°pida a nuevos ataques
  - Offline RL para aprendizaje seguro desde logs
  
- **Explicabilidad y confianza**
  - SHAP/LIME para explicar decisiones del agente
  - Certificaci√≥n de robustez formal
  - Auditor√≠a de decisiones para compliance

## üí° Consejos y Mejores Pr√°cticas

### Optimizaci√≥n del Entrenamiento
1. **Usa GPU si est√° disponible**: El entrenamiento puede ser 5-10x m√°s r√°pido
   ```python
   model = DQN("MlpPolicy", env, device="cuda", ...)
   ```

2. **Ajusta buffer_size seg√∫n memoria**: 
   - GPU 8GB: buffer_size=100_000
   - GPU 16GB+: buffer_size=500_000
   - Solo CPU: buffer_size=50_000

3. **Monitoriza el aprendizaje**:
   ```python
   from stable_baselines3.common.callbacks import EvalCallback
   
   eval_callback = EvalCallback(
       eval_env,
       best_model_save_path="./logs/best_model",
       log_path="./logs/results",
       eval_freq=10_000,
   )
   model.learn(total_timesteps=1_000_000, callback=eval_callback)
   ```

### Debugging y Desarrollo
1. **Empieza con subset peque√±o**: Usa `use_20_percent=True` para iterar r√°pido
2. **Reduce timesteps inicialmente**: Prueba con 50k-100k antes de 1M
3. **Visualiza la matriz de confusi√≥n**: Identifica si el agente est√° sesgado
4. **Registra las recompensas**: Aseg√∫rate de que aumentan durante el entrenamiento

### Reproducibilidad
```python
import numpy as np
import random
import torch

# Fijar semillas para reproducibilidad
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed(SEED)

# Pasar seed al modelo
model = DQN("MlpPolicy", env, seed=SEED, ...)
```

---

## üîß Troubleshooting

### Problemas Comunes

#### Error: "kagglehub requiere autenticaci√≥n"
**Soluci√≥n**: Configura las credenciales de Kaggle:
1. Crea cuenta en [Kaggle](https://www.kaggle.com/)
2. Ve a Settings ‚Üí API ‚Üí "Create New API Token"
3. Coloca `kaggle.json` en `~/.kaggle/` (Linux/Mac) o `C:\Users\<usuario>\.kaggle\` (Windows)
4. Permisos: `chmod 600 ~/.kaggle/kaggle.json`

#### Error: "CUDA out of memory"
**Soluci√≥n**: Reduce el tama√±o del batch o buffer:
```python
model = DQN(
    "MlpPolicy",
    env,
    buffer_size=50_000,  # Reducir de 100k
    batch_size=32,       # Reducir de 64
    device="cpu",        # O usar CPU
)
```

#### Advertencia: "No GPU detected"
**Verificar CUDA**:
```bash
python -c "import torch; print(torch.cuda.is_available())"
```
Si devuelve `False`:
- Instala PyTorch con CUDA: https://pytorch.org/get-started/locally/
- Verifica drivers NVIDIA: `nvidia-smi`

#### Error: "FileNotFoundError: KDDTrain+.txt"
**Soluci√≥n**: El dataset no se descarg√≥ correctamente:
```python
# Fuerza descarga manual
import kagglehub
path = kagglehub.dataset_download("hassan06/nslkdd")
print(f"Dataset en: {path}")
```

#### El agente no aprende (accuracy estancada)
**Posibles causas**:
1. **Learning rate muy alto/bajo**: Prueba `1e-4` o `5e-4`
2. **Reward mal dise√±ada**: Verifica que las penalizaciones no dominen
3. **Exploration insuficiente**: Aumenta `exploration_fraction` en DQN
4. **Dataset no balanceado**: Considera `class_weight` en RF o ajusta rewards

### Performance Benchmarks

Tiempos aproximados en diferentes configuraciones:

| Configuraci√≥n | Dataset | Timesteps | Tiempo Entrenamiento | Accuracy Test |
|---------------|---------|-----------|---------------------|---------------|
| CPU (8 cores) | 20% | 500k | ~45 min | ~76% |
| CPU (8 cores) | Full | 1M | ~3 horas | ~72% |
| GPU (RTX 3070) | 20% | 500k | ~12 min | ~76% |
| GPU (RTX 3070) | Full | 1M | ~30 min | ~72% |
| GPU (A100) | Full | 1M | ~15 min | ~72% |

**Nota**: Los tiempos var√≠an seg√∫n hardware, sistema operativo y carga del sistema.

## ü§ù Contribuciones

Este es un proyecto acad√©mico (TFG), pero se aceptan sugerencias y mejoras:

1. **Fork del repositorio**: Haz una copia en tu cuenta de GitHub
2. **Crea una rama**: `git checkout -b feature/nueva-mejora`
3. **Implementa cambios**: 
   - Sigue el estilo de c√≥digo existente
   - A√±ade docstrings a funciones nuevas
   - Comenta c√≥digo complejo
4. **Prueba tus cambios**: Verifica que funcionen correctamente
5. **Commit**: `git commit -m "feat: descripci√≥n clara del cambio"`
6. **Push**: `git push origin feature/nueva-mejora`
7. **Pull Request**: Abre PR con descripci√≥n detallada

### √Åreas de Contribuci√≥n Sugeridas
- üÜï Nuevos algoritmos RL (PPO, SAC, TD3)
- üìä Visualizaciones de aprendizaje (TensorBoard, W&B)
- üî¨ Experimentos con otros datasets (UNSW-NB15, CICIDS2017)
- üìù Mejoras en documentaci√≥n
- üêõ Correcci√≥n de bugs
- ‚ö° Optimizaciones de rendimiento
- üß™ Casos de test unitarios

---

## ‚ùì FAQ (Preguntas Frecuentes)

### ¬øPor qu√© usar RL en lugar de ML supervisado tradicional?

**Ventajas del RL**:
- **Optimizaci√≥n de objetivos complejos**: Puedes optimizar directamente el trade-off entre FP y FN mediante recompensas
- **Aprendizaje secuencial**: El agente considera consecuencias a largo plazo, no solo decisiones puntuales
- **Adaptabilidad**: Puede aprender online de nuevos datos sin re-entrenar desde cero
- **Flexibilidad**: F√°cil ajustar el comportamiento cambiando recompensas sin re-entrenar modelos

**Desventajas**:
- M√°s complejo de implementar y debuggear
- Requiere m√°s tiempo de entrenamiento
- M√°s dif√≠cil de interpretar que modelos como Random Forest

### ¬øEl agente funciona en tiempo real?

**No en la versi√≥n actual**. Esta es la Fase 1 (entorno simulado con dataset hist√≥rico). El agente:
- ‚úÖ Funciona: Con dataset NSL-KDD preprocesado
- ‚ùå No funciona: Con captura de tr√°fico en vivo (pcap, Wireshark)

La Fase 2 incluir√° captura en tiempo real y extracci√≥n de caracter√≠sticas online.

### ¬øQu√© tipos de ataques detecta?

NSL-KDD incluye 4 categor√≠as principales:
- **DoS** (Denial of Service): neptune, smurf, pod, teardrop, land, back
- **Probe** (Reconnaissance): portsweep, ipsweep, nmap, satan
- **R2L** (Remote to Local): guess_passwd, ftp_write, imap, phf, multihop, warezmaster, warezclient, spy
- **U2R** (User to Root): buffer_overflow, loadmodule, perl, rootkit

Actualmente el modelo solo clasifica binario (normal vs. ataque), no distingue entre tipos.

### ¬øPuedo usar mis propios datos?

**S√≠**, pero requiere adaptaci√≥n. Necesitas:

1. **Formato correcto**: Array NumPy con shape `(n_samples, n_features)`
2. **Etiquetas binarias**: 0 = normal, 1 = ataque
3. **Preprocesamiento**: One-hot encoding de categ√≥ricas, normalizaci√≥n si es necesario

Ejemplo:
```python
from rl_defender_env import RLDatasetDefenderEnv

# Tus datos (n_samples, n_features)
X_train = np.load("mi_dataset_X.npy")
y_train = np.load("mi_dataset_y.npy")

# Crear entorno
env = RLDatasetDefenderEnv(X=X_train, y=y_train)

# Entrenar como de costumbre
model = DQN("MlpPolicy", env, ...)
```

### ¬øC√≥mo elijo los hiperpar√°metros √≥ptimos?

Recomendaciones basadas en los experimentos:

**Para empezar (baseline)**:
```python
learning_rate = 1e-3
buffer_size = 100_000
batch_size = 64
total_timesteps = 500_000
```

**Para optimizar**:
1. **Grid search manual**: Prueba combinaciones y documenta en `experiments/`
2. **Optuna**: Framework de optimizaci√≥n de hiperpar√°metros
   ```bash
   pip install optuna
   ```
3. **Ray Tune**: Para b√∫squeda distribuida a gran escala

**Tip**: Empieza con pocos timesteps (50k) para iterar r√°pido, luego entrena m√°s.

### ¬øEl modelo es robusto contra ataques adversariales?

**No est√° evaluado**. Los ataques adversariales contra modelos de ML/RL en ciberseguridad incluyen:

- **Evasion attacks**: Modificar tr√°fico malicioso para parecer benigno
- **Poisoning attacks**: Inyectar datos maliciosos durante entrenamiento
- **Model extraction**: Robar el modelo mediante queries

La **Fase 3** incluir√° evaluaci√≥n de robustez adversarial.

### ¬øCu√°nta memoria RAM/GPU necesito?

**M√≠nimos**:
- RAM: 8 GB (dataset 20%)
- GPU: Opcional, pero acelera 3-5x (4 GB VRAM suficiente)

**Recomendados**:
- RAM: 16 GB (dataset completo)
- GPU: 8 GB VRAM (NVIDIA RTX 3060/4060 o superior)

**Sin GPU**: Todo funciona en CPU, solo ser√° m√°s lento.

### ¬øPuedo usar otros datasets de ciberseguridad?

**S√≠**, algunos datasets compatibles:

| Dataset | A√±o | Samples | Features | Tipos de Ataque |
|---------|-----|---------|----------|-----------------|
| NSL-KDD | 2009 | 148k | 41 | DoS, Probe, R2L, U2R |
| UNSW-NB15 | 2015 | 2.5M | 49 | 9 tipos modernos |
| CICIDS2017 | 2017 | 2.8M | 80+ | 14 tipos (web attacks, botnet) |
| CSE-CIC-IDS2018 | 2018 | 16M | 80+ | 15 tipos |

Para usar otros datasets, adapta `load_nsl_kdd.py` o crea tu propio loader.

## üìÑ Licencia

Este proyecto es de c√≥digo abierto y est√° disponible bajo la licencia MIT (o la que corresponda).

---

## üìß Contacto

- **Autor**: Disponible a trav√©s de GitHub
- **Issues**: Para reportar bugs o sugerir mejoras, abre un [Issue](https://github.com/yeaight7/TFG_CYBER_AI/issues)
- **Discusiones**: Para preguntas generales, usa [Discussions](https://github.com/yeaight7/TFG_CYBER_AI/discussions)

Para preguntas o colaboraciones, contacta con el autor del TFG a trav√©s de GitHub.

---

## üôè Agradecimientos

- **NSL-KDD Dataset**: Creado por el Canadian Institute for Cybersecurity
- **Stable-Baselines3**: Biblioteca de algoritmos RL de alta calidad
- **Gymnasium**: Framework est√°ndar para entornos RL
- **Kaggle**: Plataforma para compartir datasets

---

## üìö Referencias

- [NSL-KDD Dataset](https://www.unb.ca/cic/datasets/nsl.html)
- [Stable-Baselines3 Documentation](https://stable-baselines3.readthedocs.io/)
- [Gymnasium Documentation](https://gymnasium.farama.org/)
- [Deep Q-Network (DQN) Paper](https://www.nature.com/articles/nature14236)
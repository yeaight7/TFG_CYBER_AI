# TFG â€“ Agente de Ciberseguridad con Aprendizaje por Refuerzo

Este repositorio contiene la primera fase de un Trabajo Fin de Grado orientado al diseÃ±o de un **agente defensor** basado en **Aprendizaje por Refuerzo (Reinforcement Learning, RL)** para tareas de detecciÃ³n y bloqueo de trÃ¡fico malicioso.

En esta fase el entorno es **simulado / tipo dataset**: el agente recibe caracterÃ­sticas de flujos de red (u otras muestras etiquetadas como benignas o maliciosas) y aprende una polÃ­tica para **permitir o bloquear** el trÃ¡fico maximizando una funciÃ³n de recompensa.

## ğŸ¯ Objetivo del Proyecto

El objetivo principal es desarrollar un sistema de defensa inteligente basado en RL que pueda:

- **Detectar automÃ¡ticamente** trÃ¡fico malicioso en redes
- **Aprender polÃ­ticas Ã³ptimas** de bloqueo/permisiÃ³n mediante recompensas
- **Minimizar falsos positivos** (bloquear trÃ¡fico legÃ­timo)
- **Minimizar falsos negativos** (permitir ataques)
- **Generalizar** a nuevos tipos de ataques no vistos durante el entrenamiento

## ğŸ—ï¸ Arquitectura del Sistema

El sistema se compone de tres componentes principales:

### 1. **Entorno RL Custom (Gymnasium)**
- Implementado en `rl_defender_env.py`
- Basado en el framework Gymnasium (sucesor de OpenAI Gym)
- **Espacio de observaciÃ³n**: Vector de caracterÃ­sticas de flujos de red (multidimensional)
- **Espacio de acciones**: Discreto (0 = PERMIT, 1 = BLOCK)
- **Sistema de recompensas**:
  - Bloquear ataque correctamente: +1.0 (recompensa)
  - Permitir trÃ¡fico benigno: +0.5 (recompensa menor)
  - Bloquear trÃ¡fico benigno (FP): -1.0 (penalizaciÃ³n)
  - Permitir ataque (FN): -5.0 (penalizaciÃ³n fuerte)

### 2. **Agente RL (DQN)**
- Algoritmo: **Deep Q-Network (DQN)** de Stable-Baselines3
- PolÃ­tica: MLP (Multi-Layer Perceptron)
- Red neuronal profunda que aprende valores Q(s,a) para cada par estado-acciÃ³n
- Utiliza replay buffer y target network para estabilizar el entrenamiento

### 3. **Dataset: NSL-KDD**
- VersiÃ³n mejorada del dataset KDD Cup 1999
- Contiene flujos de red con caracterÃ­sticas como:
  - DuraciÃ³n de conexiÃ³n
  - Tipo de protocolo (TCP, UDP, ICMP)
  - Servicio de red (HTTP, FTP, SSH, etc.)
  - Flags de conexiÃ³n
  - Bytes enviados/recibidos
  - Tasas de error
  - Y 41 caracterÃ­sticas mÃ¡s
- Etiquetas: Normal vs. Ataques (DoS, Probe, R2L, U2R)
- Descarga automÃ¡tica vÃ­a `kagglehub` desde Kaggle

---

## ğŸ“ Estructura del Proyecto

```text
TFG_CYBER_AI/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ rl_defender_env.py       # Entorno Gymnasium personalizado
â”‚   â”œâ”€â”€ train_rl_defender.py      # Script principal de entrenamiento
â”‚   â””â”€â”€ load_nsl_kdd.py           # Utilidad para cargar y preprocesar NSL-KDD
â”‚
â”œâ”€â”€ datasets/
â”‚   â””â”€â”€ nsl_kdd/                  # Dataset NSL-KDD (descargado automÃ¡ticamente)
â”‚       â”œâ”€â”€ KDDTrain+.txt         # Conjunto de entrenamiento completo
â”‚       â”œâ”€â”€ KDDTrain+_20Percent.txt  # VersiÃ³n reducida (20%)
â”‚       â”œâ”€â”€ KDDTest+.txt          # Conjunto de prueba
â”‚       â””â”€â”€ ...
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ rl_defender_dqn.zip       # Modelo DQN entrenado (guardado)
â”‚
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

---

## ğŸš€ InstalaciÃ³n y ConfiguraciÃ³n

### Requisitos Previos

- **Python 3.8+** (recomendado 3.9 o 3.10)
- **pip** (gestor de paquetes de Python)
- ConexiÃ³n a internet (para descargar el dataset NSL-KDD desde Kaggle)

### Paso 1: Clonar el Repositorio

```bash
git clone https://github.com/yeaight7/TFG_CYBER_AI.git
cd TFG_CYBER_AI
```

### Paso 2: Crear Entorno Virtual

```bash
python3 -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate
```

### Paso 3: Instalar Dependencias

```bash
pip install --upgrade pip
pip install numpy pandas scikit-learn
pip install gymnasium
pip install stable-baselines3
pip install kagglehub
```

**Dependencias principales:**
- `numpy`: CÃ¡lculos numÃ©ricos
- `pandas`: ManipulaciÃ³n de datos
- `scikit-learn`: Preprocesamiento y mÃ©tricas
- `gymnasium`: Framework de entornos RL
- `stable-baselines3`: Implementaciones de algoritmos RL (DQN, PPO, A2C, etc.)
- `kagglehub`: Descarga automÃ¡tica de datasets de Kaggle

### Paso 4: Configurar Kaggle (Opcional)

Si es la primera vez usando `kagglehub`, podrÃ­a pedirte credenciales de Kaggle:

1. Crea una cuenta en [Kaggle](https://www.kaggle.com/)
2. Ve a tu perfil â†’ Settings â†’ API â†’ "Create New API Token"
3. Descarga el archivo `kaggle.json`
4. ColÃ³calo en `~/.kaggle/kaggle.json` (Linux/Mac) o `C:\Users\<usuario>\.kaggle\kaggle.json` (Windows)

---

## ğŸ“Š Dataset: NSL-KDD

### DescripciÃ³n

NSL-KDD es un dataset de detecciÃ³n de intrusiones derivado del KDD Cup 1999. Contiene registros de conexiones de red con:

- **125,973 muestras de entrenamiento** (KDDTrain+.txt)
- **22,544 muestras de prueba** (KDDTest+.txt)
- **41 caracterÃ­sticas numÃ©ricas/categÃ³ricas** por muestra
- **Etiquetas**: Normal, DoS, Probe, R2L, U2R

### Preprocesamiento AutomÃ¡tico

El script `load_nsl_kdd.py` realiza:

1. **Descarga automÃ¡tica** desde Kaggle
2. **One-hot encoding** de variables categÃ³ricas (protocol_type, service, flag)
3. **BinarizaciÃ³n de etiquetas**: 0 = Normal, 1 = Ataque
4. **DivisiÃ³n train/test** manteniendo la proporciÃ³n original
5. **ConversiÃ³n a arrays NumPy** (float32) para eficiencia

### Uso

```python
from load_nsl_kdd import load_nsl_kdd_binary

# Cargar dataset completo
X_train, y_train, X_test, y_test = load_nsl_kdd_binary(use_20_percent=False)

# O usar versiÃ³n reducida (20%) para pruebas rÃ¡pidas
X_train, y_train, X_test, y_test = load_nsl_kdd_binary(use_20_percent=True)
```

---

## ğŸ“ Entrenamiento del Agente

### EjecuciÃ³n BÃ¡sica

```bash
cd src
python train_rl_defender.py
```

### Proceso de Entrenamiento

El script `train_rl_defender.py` realiza:

1. **Carga del dataset NSL-KDD**
   ```
   Train shape: X=(125973, 122), y=(125973,)
   Test shape:  X=(22544, 122), y=(22544,)
   ```

2. **CreaciÃ³n del entorno RL**
   - Entorno personalizado `RLDatasetDefenderEnv`
   - Envuelto en `DummyVecEnv` para compatibilidad con SB3

3. **InicializaciÃ³n del modelo DQN**
   - PolÃ­tica: MLP con capas ocultas
   - Learning rate: 1e-3
   - Buffer size: 100,000 transiciones
   - Batch size: 64
   - Gamma (descuento): 0.99
   - Target network update: cada 10,000 pasos

4. **Entrenamiento**
   - Total timesteps: 1,000,000
   - Cada episodio recorre hasta 10,000 muestras
   - El agente aprende de interacciones repetidas

5. **Guardado del modelo**
   - Archivo: `models/rl_defender_dqn.zip`

6. **EvaluaciÃ³n en test**
   - MÃ©tricas: Matriz de confusiÃ³n, Precision, Recall, F1-Score
   - AcciÃ³n 0 = PERMIT, AcciÃ³n 1 = BLOCK

### ParÃ¡metros Configurables

Puedes modificar hiperparÃ¡metros en `train_rl_defender.py`:

```python
model = DQN(
    "MlpPolicy",
    env,
    learning_rate=1e-3,        # Tasa de aprendizaje
    buffer_size=100_000,        # TamaÃ±o del replay buffer
    batch_size=64,              # TamaÃ±o del batch
    gamma=0.99,                 # Factor de descuento
    tau=1.0,                    # Tasa de actualizaciÃ³n de target network
    train_freq=4,               # Frecuencia de entrenamiento
    target_update_interval=10_000,  # Intervalo de actualizaciÃ³n
    verbose=1,
)
```

---

## ğŸ§ª EvaluaciÃ³n y MÃ©tricas

### Matriz de ConfusiÃ³n

```
                Predicho PERMIT (0)  Predicho BLOCK (1)
Real Normal (0)        TP              FP
Real Ataque (1)        FN              TP
```

### MÃ©tricas Clave

- **Precision**: TP / (TP + FP) - ProporciÃ³n de bloqueos correctos
- **Recall**: TP / (TP + FN) - ProporciÃ³n de ataques detectados
- **F1-Score**: Media armÃ³nica de Precision y Recall
- **Accuracy**: (TP + TN) / Total - ProporciÃ³n total de aciertos

### Ejemplo de Salida

```
=== Confusion matrix (acciones: 0=PERMIT, 1=BLOCK) ===
[[  9711    644]
 [  1544  10645]]

=== Classification report ===
              precision    recall  f1-score   support

           0     0.8630    0.9378    0.8988     10355
           1     0.9429    0.8733    0.9068     12189

    accuracy                         0.9030     22544
   macro avg     0.9030    0.9056    0.9028     22544
weighted avg     0.9061    0.9030    0.9034     22544
```

---

## ğŸ”¬ ExperimentaciÃ³n

### Probar Diferentes Algoritmos RL

AdemÃ¡s de DQN, puedes experimentar con:

```python
from stable_baselines3 import PPO, A2C, SAC

# PPO (Proximal Policy Optimization)
model = PPO("MlpPolicy", env, verbose=1)

# A2C (Advantage Actor-Critic)
model = A2C("MlpPolicy", env, verbose=1)
```

### Ajustar Sistema de Recompensas

En `rl_defender_env.py`, modifica:

```python
RLDatasetDefenderEnv(
    X=X,
    y=y,
    correct_reward=2.0,              # Aumentar recompensa por aciertos
    false_positive_penalty=-0.5,     # Reducir penalizaciÃ³n de FP
    false_negative_penalty=-10.0,    # Aumentar penalizaciÃ³n de FN
    # ...
)
```

### Entrenar con Subset Reducido

Para experimentaciÃ³n rÃ¡pida:

```python
# En train_rl_defender.py
X_train, y_train, X_test, y_test = load_nsl_kdd_binary(
    use_20_percent=True  # Solo 20% del dataset
)
```

---

## ğŸ“ˆ Cargar Modelo Pre-entrenado

Para evaluar o continuar el entrenamiento:

```python
from stable_baselines3 import DQN

# Cargar modelo guardado
model = DQN.load("models/rl_defender_dqn")

# Evaluar en nuevo conjunto
obs, info = env.reset()
action, _ = model.predict(obs, deterministic=True)
```

---

## ğŸ”® Trabajo Futuro

Esta es la **Fase 1** del TFG. Las siguientes fases incluirÃ¡n:

### Fase 2: Entorno en Tiempo Real
- IntegraciÃ³n con captura de trÃ¡fico en vivo (pcap, Wireshark)
- Uso de herramientas como Scapy para anÃ¡lisis de paquetes
- Pipeline de procesamiento en streaming

### Fase 3: Adversario RL
- Implementar un **agente atacante** tambiÃ©n basado en RL
- Escenario de juego adversarial (Game Theory)
- Co-evoluciÃ³n defensor vs. atacante

### Fase 4: Multi-Agente
- Sistema distribuido con mÃºltiples defensores
- CoordinaciÃ³n y comunicaciÃ³n entre agentes
- Defensa de redes complejas

### Fase 5: Despliegue
- ContenedorizaciÃ³n (Docker)
- IntegraciÃ³n con firewalls (iptables, nftables)
- Dashboard de monitorizaciÃ³n

---

## ğŸ¤ Contribuciones

Este es un proyecto acadÃ©mico (TFG), pero se aceptan sugerencias y mejoras:

1. Haz fork del repositorio
2. Crea una rama para tu feature (`git checkout -b feature/mejora`)
3. Commit tus cambios (`git commit -am 'AÃ±adir mejora X'`)
4. Push a la rama (`git push origin feature/mejora`)
5. Abre un Pull Request

---

## ğŸ“„ Licencia

Este proyecto es de cÃ³digo abierto y estÃ¡ disponible bajo la licencia MIT (o la que corresponda).

---

## ğŸ“§ Contacto

Para preguntas o colaboraciones, contacta con el autor del TFG a travÃ©s de GitHub.

---

## ğŸ™ Agradecimientos

- **NSL-KDD Dataset**: Creado por el Canadian Institute for Cybersecurity
- **Stable-Baselines3**: Biblioteca de algoritmos RL de alta calidad
- **Gymnasium**: Framework estÃ¡ndar para entornos RL
- **Kaggle**: Plataforma para compartir datasets

---

## ğŸ“š Referencias

- [NSL-KDD Dataset](https://www.unb.ca/cic/datasets/nsl.html)
- [Stable-Baselines3 Documentation](https://stable-baselines3.readthedocs.io/)
- [Gymnasium Documentation](https://gymnasium.farama.org/)
- [Deep Q-Network (DQN) Paper](https://www.nature.com/articles/nature14236)